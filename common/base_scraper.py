# data-pipeline/common/base_scraper.py
"""Base scraper class with robust error handling and retry logic."""

import time
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import logging

from .config import OUTPUTS_DIR, LOGS_DIR
from .utils import (
    clean_data,
    get_file_size_mb,
    setup_logger,
    validate_date_range,
    ValidationError,
)


class ScraperError(Exception):
    """ExcepciÃ³n base para errores del scraper."""
    pass


class ScrapingError(ScraperError):
    """ExcepciÃ³n para errores durante el scraping."""
    pass


class ProcessingError(ScraperError):
    """ExcepciÃ³n para errores durante el procesamiento."""
    pass


class SavingError(ScraperError):
    """ExcepciÃ³n para errores durante el guardado."""
    pass


class BaseScraper(ABC):
    """
    Clase abstracta base para todos los scrapers de loterÃ­as de animalitos.
    
    Define un flujo de trabajo estÃ¡ndar en 3 fases: scrape â†’ process â†’ save.
    Incluye manejo robusto de errores, retry logic, validaciones y mÃ©tricas.
    """

    def __init__(
        self, 
        name: str, 
        url: str,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        timeout: int = 30,
        max_data_size_mb: float = 100.0
    ):
        """
        Inicializa el scraper base.
        
        Args:
            name: Nombre Ãºnico del scraper
            url: URL base para el scraping
            max_retries: NÃºmero mÃ¡ximo de reintentos
            retry_delay: Delay entre reintentos en segundos
            timeout: Timeout para requests en segundos
            max_data_size_mb: TamaÃ±o mÃ¡ximo de datos en MB
        """
        self.name = name
        self.url = url
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.timeout = timeout
        self.max_data_size_mb = max_data_size_mb
        
        # Datos del scraper
        self.raw_data: List[Dict[str, Any]] = []
        self.processed_data: List[Dict[str, Any]] = []
        
        # MÃ©tricas
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None
        self.total_records: int = 0
        self.successful_records: int = 0
        self.failed_records: int = 0
        
        # Configurar logger
        self.logger = self._setup_scraper_logger()
        
        # Validar configuraciÃ³n
        self._validate_configuration()

    def _setup_scraper_logger(self) -> logging.Logger:
        """Configura un logger especÃ­fico para este scraper."""
        log_file = LOGS_DIR / f"{self.name}.log"
        return setup_logger(self.name, log_file)

    def _validate_configuration(self) -> None:
        """Valida la configuraciÃ³n del scraper."""
        if not self.name or not isinstance(self.name, str):
            raise ValidationError("El nombre del scraper debe ser una cadena no vacÃ­a")
        
        if not self.url or not isinstance(self.url, str):
            raise ValidationError("La URL debe ser una cadena no vacÃ­a")
        
        # Validar URL
        try:
            parsed_url = urlparse(self.url)
            if not parsed_url.scheme or not parsed_url.netloc:
                raise ValidationError(f"URL invÃ¡lida: {self.url}")
        except Exception as e:
            raise ValidationError(f"Error al validar URL: {str(e)}")
        
        if self.max_retries < 0:
            raise ValidationError("max_retries debe ser >= 0")
        
        if self.retry_delay < 0:
            raise ValidationError("retry_delay debe ser >= 0")
        
        if self.timeout <= 0:
            raise ValidationError("timeout debe ser > 0")
        
        if self.max_data_size_mb <= 0:
            raise ValidationError("max_data_size_mb debe ser > 0")

    # ------------------------
    # MÃ©todos abstractos
    # ------------------------
    @abstractmethod
    def scrape_data(self, start_date: str, end_date: str) -> List[Dict[str, Any]]:
        """
        Extrae los datos crudos de la pÃ¡gina web.
        
        Args:
            start_date: Fecha de inicio (YYYY-MM-DD)
            end_date: Fecha de fin (YYYY-MM-DD)
            
        Returns:
            Lista de resultados crudos
            
        Raises:
            ScrapingError: Si hay error durante el scraping
        """
        pass

    @abstractmethod
    def process_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Procesa y limpia los datos crudos.
        
        Args:
            raw_data: Datos sin procesar
            
        Returns:
            Lista de resultados procesados
            
        Raises:
            ProcessingError: Si hay error durante el procesamiento
        """
        pass

    @abstractmethod
    def save_data(self, processed_data: List[Dict[str, Any]], output_format: str = "json") -> Path:
        """
        Guarda los datos procesados en el formato deseado.
        
        Args:
            processed_data: Lista procesada y validada
            output_format: Formato de salida ("json", "csv", etc.)
            
        Returns:
            Path del archivo guardado
            
        Raises:
            SavingError: Si hay error durante el guardado
        """
        pass

    # ------------------------
    # Flujo principal
    # ------------------------
    def run(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """
        Orquesta el flujo completo del scraper con manejo robusto de errores.
        
        Args:
            start_date: Fecha de inicio (YYYY-MM-DD)
            end_date: Fecha de fin (YYYY-MM-DD)
            
        Returns:
            Diccionario con mÃ©tricas y resultados del scraping
            
        Raises:
            ScraperError: Si hay error crÃ­tico durante la ejecuciÃ³n
        """
        self.start_time = datetime.now()
        self.logger.info(f"ðŸš€ Iniciando scraping para {self.name}")
        self.logger.info(f"ðŸ“… Rango de fechas: {start_date} â†’ {end_date}")
        
        try:
            # Validar fechas
            if not validate_date_range(start_date, end_date):
                raise ValidationError(f"Rango de fechas invÃ¡lido: {start_date} â†’ {end_date}")
            
            # Ejecutar pasos con retry logic
            raw_data = self._scrape_step_with_retry(start_date, end_date)
            processed_data = self._process_step_with_retry(raw_data)
            output_file = self._save_step_with_retry(processed_data)
            
            # Calcular mÃ©tricas finales
            self.end_time = datetime.now()
            metrics = self._calculate_metrics(output_file)
            
            self.logger.info(f"ðŸ Flujo completado con Ã©xito para {self.name}")
            self.logger.info(f"ðŸ“Š MÃ©tricas: {metrics}")
            
            return metrics
            
        except Exception as e:
            self.end_time = datetime.now()
            error_msg = f"Error durante la ejecuciÃ³n: {str(e)}"
            self.logger.error(f"ðŸ’¥ {error_msg}", exc_info=True)
            raise ScraperError(error_msg) from e

    # ------------------------
    # Pasos con retry logic
    # ------------------------
    def _scrape_step_with_retry(self, start_date: str, end_date: str) -> List[Dict[str, Any]]:
        """Paso 1: Scraping con retry logic."""
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                self.logger.info(f"ðŸ“¥ Intento {attempt + 1} de scraping")
                
                data = self.scrape_data(start_date, end_date)
                
                if not data:
                    self.logger.warning("ðŸ“­ No se encontraron datos en el rango especificado")
                    return []
                
                # Validar tamaÃ±o de datos
                data_size = len(str(data)) / (1024 * 1024)  # AproximaciÃ³n en MB
                if data_size > self.max_data_size_mb:
                    raise ScrapingError(
                        f"Datos demasiado grandes: {data_size:.2f}MB > {self.max_data_size_mb}MB"
                    )
                
                self.raw_data = data
                self.total_records = len(data)
                self.logger.info(f"âœ… {len(data)} registros extraÃ­dos exitosamente")
                return data
                
            except Exception as e:
                last_error = e
                self.logger.warning(f"âš  Intento {attempt + 1} fallÃ³: {str(e)}")
                
                if attempt < self.max_retries:
                    delay = self.retry_delay * (2 ** attempt)  # Backoff exponencial
                    self.logger.info(f"â³ Reintentando en {delay:.1f} segundos...")
                    time.sleep(delay)
                else:
                    self.logger.error("âŒ Todos los intentos de scraping fallaron")
        
        raise ScrapingError(f"Scraping fallÃ³ despuÃ©s de {self.max_retries + 1} intentos") from last_error

    def _process_step_with_retry(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Paso 2: Procesamiento con retry logic."""
        if not raw_data:
            return []
        
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                self.logger.info(f"ðŸ”„ Intento {attempt + 1} de procesamiento")
                
                processed = self.process_data(raw_data)
                
                if not processed:
                    self.logger.warning("âš  No se pudieron procesar los datos")
                    return []
                
                # Limpiar datos
                cleaned_data = clean_data(processed)
                
                self.processed_data = cleaned_data
                self.successful_records = len(cleaned_data)
                self.failed_records = len(raw_data) - len(cleaned_data)
                
                self.logger.info(f"âœ… {len(cleaned_data)} registros procesados exitosamente")
                return cleaned_data
                
            except Exception as e:
                last_error = e
                self.logger.warning(f"âš  Intento {attempt + 1} de procesamiento fallÃ³: {str(e)}")
                
                if attempt < self.max_retries:
                    delay = self.retry_delay * (2 ** attempt)
                    self.logger.info(f"â³ Reintentando en {delay:.1f} segundos...")
                    time.sleep(delay)
                else:
                    self.logger.error("âŒ Todos los intentos de procesamiento fallaron")
        
        raise ProcessingError(f"Procesamiento fallÃ³ despuÃ©s de {self.max_retries + 1} intentos") from last_error

    def _save_step_with_retry(self, processed_data: List[Dict[str, Any]]) -> Path:
        """Paso 3: Guardado con retry logic."""
        if not processed_data:
            self.logger.warning("âš  No hay datos procesados para guardar")
            return None
        
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                self.logger.info(f"ðŸ’¾ Intento {attempt + 1} de guardado")
                
                output_file = self.save_data(processed_data, output_format="json")
                
                if output_file and output_file.exists():
                    file_size = get_file_size_mb(output_file)
                    self.logger.info(f"âœ… Datos guardados en {output_file} ({file_size}MB)")
                    return output_file
                else:
                    raise SavingError("El archivo no se creÃ³ correctamente")
                
            except Exception as e:
                last_error = e
                self.logger.warning(f"âš  Intento {attempt + 1} de guardado fallÃ³: {str(e)}")
                
                if attempt < self.max_retries:
                    delay = self.retry_delay * (2 ** attempt)
                    self.logger.info(f"â³ Reintentando en {delay:.1f} segundos...")
                    time.sleep(delay)
                else:
                    self.logger.error("âŒ Todos los intentos de guardado fallaron")
        
        raise SavingError(f"Guardado fallÃ³ despuÃ©s de {self.max_retries + 1} intentos") from last_error

    # ------------------------
    # MÃ©todos de utilidad
    # ------------------------
    def _calculate_metrics(self, output_file: Optional[Path]) -> Dict[str, Any]:
        """Calcula mÃ©tricas del scraping."""
        duration = None
        if self.start_time and self.end_time:
            duration = (self.end_time - self.start_time).total_seconds()
        
        metrics = {
            "scraper_name": self.name,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "duration_seconds": duration,
            "total_records": self.total_records,
            "successful_records": self.successful_records,
            "failed_records": self.failed_records,
            "success_rate": (
                self.successful_records / self.total_records 
                if self.total_records > 0 else 0
            ),
            "output_file": str(output_file) if output_file else None,
            "output_file_size_mb": get_file_size_mb(output_file) if output_file else 0,
        }
        
        return metrics

    def get_status(self) -> Dict[str, Any]:
        """Obtiene el estado actual del scraper."""
        return {
            "name": self.name,
            "url": self.url,
            "raw_data_count": len(self.raw_data),
            "processed_data_count": len(self.processed_data),
            "is_running": self.start_time is not None and self.end_time is None,
            "last_run": self.end_time.isoformat() if self.end_time else None,
        }

    def reset(self) -> None:
        """Resetea el estado del scraper."""
        self.raw_data = []
        self.processed_data = []
        self.start_time = None
        self.end_time = None
        self.total_records = 0
        self.successful_records = 0
        self.failed_records = 0
        self.logger.info("ðŸ”„ Estado del scraper reseteado")

    def validate_data_quality(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Valida la calidad de los datos.
        
        Args:
            data: Datos a validar
            
        Returns:
            Diccionario con mÃ©tricas de calidad
        """
        if not data:
            return {"valid": False, "issues": ["No hay datos"]}
        
        issues = []
        total_records = len(data)
        valid_records = 0
        
        for i, record in enumerate(data):
            if not isinstance(record, dict):
                issues.append(f"Registro {i} no es un diccionario")
                continue
            
            if not record:
                issues.append(f"Registro {i} estÃ¡ vacÃ­o")
                continue
            
            valid_records += 1
        
        quality_metrics = {
            "valid": len(issues) == 0,
            "total_records": total_records,
            "valid_records": valid_records,
            "invalid_records": total_records - valid_records,
            "quality_score": valid_records / total_records if total_records > 0 else 0,
            "issues": issues[:10],  # Solo los primeros 10 issues
            "total_issues": len(issues)
        }
        
        return quality_metrics