# Lotto Activo Scraper

Scraper robusto para extraer datos hist√≥ricos de la loter√≠a Lotto Activo con procesamiento autom√°tico y almacenamiento estructurado.

## üéØ Caracter√≠sticas

- **Scraping robusto**: Extracci√≥n de datos con manejo de errores y retry logic
- **Procesamiento inteligente**: Validaci√≥n y normalizaci√≥n autom√°tica de datos
- **Almacenamiento dual**: Datos en `outputs/` y `data/` para diferentes usos
- **Manejo de errores**: Recuperaci√≥n autom√°tica de fallos temporales
- **Logging detallado**: Monitoreo completo de operaciones
- **Validaci√≥n de datos**: Control de calidad y consistencia

## üìÅ Estructura de Directorios

```
lotto-activo/
‚îú‚îÄ‚îÄ scraper.py          # Implementaci√≥n principal del scraper
‚îú‚îÄ‚îÄ requirements.txt    # Dependencias del proyecto
‚îú‚îÄ‚îÄ README.md          # Documentaci√≥n
‚îú‚îÄ‚îÄ __init__.py        # M√≥dulo Python
‚îî‚îÄ‚îÄ data/              # Datos procesados para API REST
    ‚îî‚îÄ‚îÄ lotto_activo_YYYYMMDD_HHMMSS.json
```

## üöÄ Instalaci√≥n

### 1. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 2. Configurar entorno

```bash
# Crear directorios necesarios
mkdir -p outputs/lotto-activo
mkdir -p data/lotto-activo
mkdir -p logs
```

## üíª Uso

### Uso b√°sico

```python
from lotto_activo.scraper import LottoActivoScraper

# Crear instancia del scraper
scraper = LottoActivoScraper()

# Extraer datos para un rango de fechas
results = scraper.run("2025-01-01", "2025-01-07")

# Obtener datos m√°s recientes (√∫ltimos 7 d√≠as)
recent_data = scraper.get_latest_data(days=7)
```

### Configuraci√≥n avanzada

```python
# Configuraci√≥n personalizada
scraper = LottoActivoScraper(
    name="lotto-activo-custom",
    url="https://custom-url.com/lotto/{start}/{end}/",
    max_retries=5,
    retry_delay=3.0,
    timeout=60,
    max_data_size_mb=100.0
)
```

### Uso con manejo de errores

```python
from lotto_activo.scraper import LottoActivoScraper
from common.base_scraper import ScraperError

try:
    scraper = LottoActivoScraper()
    results = scraper.run("2025-01-01", "2025-01-07")
    print(f"‚úÖ Extra√≠dos {len(results)} registros")
except ScraperError as e:
    print(f"‚ùå Error: {e}")
finally:
    scraper.close()
```

## üìä Formato de Datos

### Datos de entrada (raw)
```json
{
    "fecha": "2025-01-15",
    "numero": "05",
    "animal": "LEON",
    "hora": "14:30:00",
    "fecha_raw": "15 de enero de 2025",
    "numero_raw": "5",
    "animal_raw": "Le√≥n",
    "hora_raw": "2:30 PM",
    "fila": 1
}
```

### Datos procesados (output)
```json
{
    "fecha": "2025-01-15",
    "numero": "05",
    "animal": "LEON",
    "numero_map": "05",
    "fuente": "lotto-activo",
    "scraper": "lotto-activo",
    "procesado_en": "2025-01-15T14:30:00.123456",
    "fila": 1,
    "hora": "14:30:00",
    "validado": true
}
```

## üîß Configuraci√≥n

### Variables de entorno

```bash
# Configuraci√≥n opcional
export LOTTO_ACTIVO_URL="https://custom-url.com/lotto/{start}/{end}/"
export LOTTO_ACTIVO_MAX_RETRIES="5"
export LOTTO_ACTIVO_TIMEOUT="60"
export LOTTO_ACTIVO_MAX_DATA_SIZE_MB="100"
```

### Par√°metros del scraper

| Par√°metro | Tipo | Default | Descripci√≥n |
|-----------|------|---------|-------------|
| `name` | str | "lotto-activo" | Nombre del scraper |
| `url` | str | URL por defecto | URL base con placeholders |
| `max_retries` | int | 3 | N√∫mero m√°ximo de reintentos |
| `retry_delay` | float | 2.0 | Delay entre reintentos (segundos) |
| `timeout` | int | 30 | Timeout para requests (segundos) |
| `max_data_size_mb` | float | 50.0 | Tama√±o m√°ximo de datos (MB) |

## üìà Monitoreo y Logging

### Niveles de log

- **INFO**: Operaciones normales
- **WARNING**: Problemas menores recuperables
- **ERROR**: Errores que requieren atenci√≥n
- **DEBUG**: Informaci√≥n detallada para debugging

### M√©tricas disponibles

```python
# Obtener m√©tricas del √∫ltimo scraping
metrics = scraper._calculate_metrics(output_file)
print(f"Duraci√≥n: {metrics['duration_seconds']}s")
print(f"Registros: {metrics['total_records']}")
print(f"√âxito: {metrics['success_rate']:.2%}")
```

### Estado del scraper

```python
# Verificar estado actual
status = scraper.get_status()
print(f"Estado: {status}")
```

## üß™ Testing

### Ejecutar tests

```bash
# Tests unitarios
pytest test/test_lotto_activo_scraper.py -v

# Tests con cobertura
pytest test/test_lotto_activo_scraper.py --cov=lotto_activo --cov-report=html

# Tests de integraci√≥n
pytest test/test_integration.py -v
```

### Tests disponibles

- **Test de scraping**: Verificaci√≥n de extracci√≥n de datos
- **Test de procesamiento**: Validaci√≥n de transformaci√≥n de datos
- **Test de guardado**: Verificaci√≥n de persistencia
- **Test de errores**: Manejo de casos de error
- **Test de validaci√≥n**: Control de calidad de datos

## üîç Troubleshooting

### Problemas comunes

#### 1. Error de conexi√≥n
```
ScrapingError: Error de red al consultar URL
```
**Soluci√≥n**: Verificar conectividad y URL, ajustar timeout

#### 2. Datos no encontrados
```
WARNING: No se encontraron datos en el rango especificado
```
**Soluci√≥n**: Verificar rango de fechas y selectores HTML

#### 3. Error de validaci√≥n
```
ValidationError: Rango de fechas inv√°lido
```
**Soluci√≥n**: Usar formato de fecha YYYY-MM-DD

### Debugging

```python
import logging

# Habilitar logging detallado
logging.basicConfig(level=logging.DEBUG)

# Usar scraper con debug
scraper = LottoActivoScraper()
scraper.logger.setLevel(logging.DEBUG)
```

## üìö API Reference

### LottoActivoScraper

#### M√©todos principales

- `run(start_date, end_date)`: Ejecuta scraping completo
- `get_latest_data(days=7)`: Obtiene datos recientes
- `close()`: Cierra sesi√≥n de requests

#### M√©todos internos

- `scrape_data(start_date, end_date)`: Extrae datos crudos
- `process_data(raw_data)`: Procesa y valida datos
- `save_data(processed_data)`: Guarda datos procesados

### Excepciones

- `ScrapingError`: Error durante extracci√≥n
- `ProcessingError`: Error durante procesamiento
- `SavingError`: Error durante guardado
- `ValidationError`: Error de validaci√≥n de datos

## ü§ù Contribuci√≥n

### Estructura del c√≥digo

1. **Scraping**: Extracci√≥n de datos de la web
2. **Procesamiento**: Validaci√≥n y normalizaci√≥n
3. **Guardado**: Persistencia en archivos
4. **Manejo de errores**: Recuperaci√≥n y logging

### Gu√≠as de desarrollo

1. Seguir PEP 8 para estilo de c√≥digo
2. Agregar tests para nuevas funcionalidades
3. Documentar cambios en README
4. Usar type hints en funciones p√∫blicas

## üìÑ Licencia

Este proyecto es parte del sistema de data pipeline para loter√≠as y est√° sujeto a las pol√≠ticas internas de la organizaci√≥n.

## üîó Enlaces relacionados

- [BaseScraper Documentation](../common/README.md)
- [Utils Documentation](../common/utils.py)
- [Config Documentation](../common/config.py)
- [Test Suite](../test/README.md)
